# app.py (Final Version with Live Model)

import gradio as gr
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import textstat
from torch.nn.utils.rnn import pad_sequence

print("--- BIA Live Inference Application ---")

# --- 1. LOAD MODEL AND TOKENIZER ---
# Define the device (use GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define the model architecture EXACTLY as we did in training.
class BIA_Model(nn.Module):
    def __init__(self, num_numerical_features, bert_model_name='distilbert-base-uncased'):
        super(BIA_Model, self).__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        self.numerical_processor = nn.Sequential(
            nn.Linear(num_numerical_features, 128),
            nn.ReLU(),
        )
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size + 128, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask, numerical_features):
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_embedding = bert_output.last_hidden_state[:, 0]
        numerical_embedding = self.numerical_processor(numerical_features)
        combined_embedding = torch.cat((text_embedding, numerical_embedding), dim=1)
        logits = self.classifier(combined_embedding)
        return logits

# Load the tokenizer
MODEL_NAME = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Instantiate the model architecture
model = BIA_Model(num_numerical_features=2)

# Load the saved weights from our trained model
MODEL_SAVE_PATH = 'bia_model.pth'
model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))

# Set the model to evaluation mode. This is important for inference.
model.to(device)
model.eval()

print("--- Model and Tokenizer Loaded Successfully ---")


# --- 2. DEFINE PREPROCESSING AND PREDICTION LOGIC ---

# We need the same formality calculation function from our preprocessing script.
def calculate_formality(text):
    try:
        reading_ease = 100 - textstat.flesch_reading_ease(text)
        avg_sentence_length = textstat.avg_sentence_length(text)
        formality_score = (reading_ease * 0.4) + (avg_sentence_length * 0.6)
        return formality_score
    except (ValueError, ZeroDivisionError):
        return 0

def predict(text1, text2, text3, time_between_posts, formality_score_manual):
    """
    Takes raw user input, preprocesses it, and returns a prediction from the model.
    """
    # For now, we'll analyze the first piece of text as a single-post sequence.
    # A more advanced implementation could analyze all three.
    text_to_analyze = text1
    
    if not text_to_analyze.strip():
        return {"Error": 1.0}, "Please provide at least one text sample for analysis."

    # --- Preprocessing ---
    # 1. Calculate formality score
    formality_score = calculate_formality(text_to_analyze)
    
    # 2. Tokenize text
    tokens = tokenizer.encode(text_to_analyze, truncation=True, max_length=512, return_tensors="pt")
    
    # 3. Prepare numerical features
    # Note: We are using the calculated formality score, not the manual one from the slider, for the model.
    # The slider is for demonstration purposes.
    numerical_features = torch.tensor([[time_between_posts, formality_score]], dtype=torch.float32)
    
    # 4. Create attention mask
    attention_mask = (tokens != tokenizer.pad_token_id).type(torch.float32)

    # Move all tensors to the correct device
    tokens = tokens.to(device)
    attention_mask = attention_mask.to(device)
    numerical_features = numerical_features.to(device)
    
    # --- Prediction ---
    # Get the model's prediction
    with torch.no_grad(): # Disable gradient calculation for faster inference
        output_logits = model(tokens, attention_mask, numerical_features)
    
    # Convert the raw output (logits) to a probability score (0 to 1)
    probability = torch.sigmoid(output_logits).squeeze().item()
    
    # --- Format Output ---
    if probability > 0.5:
        label = "Likely Synthetic"
    else:
        label = "Likely Human"
        
    output_prediction = {
        "Likely Synthetic": probability,
        "Likely Human": 1 - probability
    }
    
    explanation = (
        f"ANALYSIS COMPLETE:\n"
        f"The model predicts this communication is **{label}** with a confidence of {probability:.2%}.\n\n"
        f"Behavioral Factors Analyzed:\n"
        f"- Time Between Comms (Input): {time_between_posts} hours\n"
        f"- Calculated Formality Score: {formality_score:.2f}\n\n"
        f"This prediction is generated by a custom-trained multi-input transformer model."
    )
    
    return output_prediction, explanation


# --- 3. BUILD GRADIO UI ---
with gr.Blocks(theme=gr.themes.Monochrome(), title="BIA Engine") as demo:
    gr.Markdown(
        """
        <div style="text-align: center;">
            <h1>ðŸ§  Behavioral Identity Analysis (BIA) Engine</h1>
            <p>A proof-of-concept tool for detecting synthetic identities based on communication patterns.</p>
        </div>
        """
    )

    with gr.Tabs():
        with gr.TabItem("ðŸ”Ž Analysis Engine"):
            with gr.Row(equal_height=True):
                with gr.Column(scale=2):
                    gr.Markdown("### Step 1: Provide Communication Samples")
                    text_input_1 = gr.Textbox(label="Communication 1", placeholder="Enter the first message, email, or post...", lines=4)
                    text_input_2 = gr.Textbox(label="Communication 2 (Optional)", placeholder="Enter the second message...", lines=4)
                    text_input_3 = gr.Textbox(label="Communication 3 (Optional)", placeholder="Enter the third message...", lines=4)
                    
                    with gr.Accordion("Step 2: Add Behavioral Metadata", open=True):
                        time_input = gr.Slider(minimum=0, maximum=240, value=0, label="Time Since Last Communication (Hours)")
                        formality_input = gr.Slider(minimum=1, maximum=10, value=5, label="Estimated Formality Score (Demonstration Only)")
                    
                    analyze_button = gr.Button("Analyze Identity", variant="primary")

                with gr.Column(scale=1):
                    gr.Markdown("### Step 3: Review Analysis")
                    output_label = gr.Label(label="Prediction")
                    output_explanation = gr.Textbox(label="Model Explanation", interactive=False, lines=13)

        with gr.TabItem("ðŸ“– About this Project"):
            gr.Markdown(
                """
                ## Project: Behavioral Identity Analysis (BIA) for Synthetic Fraud Detection
                **Objective:** To differentiate between human and AI-generated text by analyzing behavioral and linguistic patterns over a *sequence* of communications.
                """
            )

    # Connect the button to our new 'predict' function
    analyze_button.click(
        fn=predict,
        inputs=[text_input_1, text_input_2, text_input_3, time_input, formality_input],
        outputs=[output_label, output_explanation]
    )

if __name__ == "__main__":
    demo.launch()